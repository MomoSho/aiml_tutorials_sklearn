{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Scikit-Learn and Pandas\n",
    "Artifical Intelligence and Machine Learning Symposium at OU  \n",
    "Univeristy of Oklahoma Memorial Union Ballroom  \n",
    "September 25, 2019  \n",
    "Instructior: Monique Shotande <monique.shotande@ou.edu>  \n",
    "\n",
    "## Overview: Classification\n",
    "Effect of Training/Test size on performance intrepretation  \n",
    "Model selection using Cross Validation  \n",
    "Making categorical predictions  \n",
    "Evaluating model's generalized performance with Cross Validation    \n",
    "\n",
    "### General References\n",
    "* [Sci-kit Learn API](https://scikit-learn.org/stable/modules/classes.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is\n",
    "# downloaded from: https://goo.gl/U2Uwz2\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import itertools \n",
    "import time\n",
    "\n",
    "from matplotlib import rcParams, pyplot as plt\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "\n",
    "globalStart = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD AND SETUP DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dat set\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Setup variables\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "feature_names = list(data.feature_names)\n",
    "target_names = list(data.target_names)\n",
    "all_varnames = feature_names + ['tumor_class']\n",
    "\n",
    "print(\"SETUP\")\n",
    "print(X.shape, y.shape)\n",
    "print(\"Features: \\n\" + str(feature_names))\n",
    "print(\"Targets: \\n\" + str(target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRIEF DATA PREPROCESSNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check data for any NaNs or infinite values\n",
    "\"\"\"\n",
    "Xy = np.append(X, y.reshape(-1,1), axis=1)\n",
    "df_Xy = pd.DataFrame(Xy, columns=all_varnames)\n",
    "print(\"Any NaNs?\", np.any(np.isnan(Xy)))\n",
    "print(\"Any Infs?\", np.any(np.isinf(Xy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRIEF DATA VISULAIZATION AND TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xy.hist(grid=False, figsize=(15,15))\n",
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xy.boxplot(vert=False, grid=False, figsize=(10,10))\n",
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Robustly scale data to mitigate for outliers\n",
    "\"\"\"\n",
    "robustscaler = RobustScaler()\n",
    "Xscaled = robustscaler.fit_transform(X)\n",
    "\n",
    "df_xs = pd.DataFrame(Xscaled, columns=feature_names)\n",
    "df_xs.hist(grid=False, figsize=(15,15))\n",
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xs.boxplot(vert=False, grid=False, figsize=(10,10))\n",
    "a = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCORE COMPUTATION FUNCTIONS\n",
    "Define a set of helper functions here that can compute various skill scores, to use when evaluating and comparing models.\n",
    "Accuracy is not always the best metric for accessing profinecy of a model, especially in the case of largely unbalanced datasets where the relative occurence of one class, is signifanctly larger than that of other classes.\n",
    "Generally, a skill score will measure the relative improvement of the forecast over some benchmark forecast.  \n",
    "\n",
    "\n",
    "$RB=\\frac{ \\sum_{t=1}^{N} (f_t-o_t) }{ \\sum_{t=1}^{N} o_t }$\n",
    "\n",
    "$BS=\\frac{1}{N} \\sum_{t=1}^{N} (f_t-o_t)^2$\n",
    "\n",
    "* $f_t$: forecast probability for instance t\n",
    "* $o_t$: actual event outcome at instance t; 0 for no occurance, 1 for an occurance\n",
    "\n",
    "\n",
    "\n",
    "$PSS=tpr-tpr= \\frac{tp}{tp+fn} - \\frac{fp}{tn+fp}$\n",
    "\n",
    "$precision= \\frac{tp}{tp+fp}$  \n",
    "* The ability of the classifier to not label negative samples as positive.\n",
    "\n",
    "$recall= \\frac{tp}{tp+fn}$  \n",
    "* The ability of the classifier to find all the positive samples.\n",
    "\n",
    "\n",
    "\n",
    "$f_1 score= 2 \\frac{precision * recall}{precision+recall}$  \n",
    "* The weighted harmonic mean of the precision and recall. Best value is 1 and worst 0.\n",
    "\n",
    "\n",
    "References:  \n",
    "[Forecast Skill](https://en.wikipedia.org/wiki/Forecast_skill)  \n",
    "[Brier Score (BS)](https://en.wikipedia.org/wiki/Brier_score)  \n",
    "[Glossary of Forecast Verification Statistics](https://www.nws.noaa.gov/oh/rfcdev/docs/Glossary_Forecast_Verification_Metrics.pdf)  \n",
    "[Scoring Rule](https://en.wikipedia.org/wiki/Scoring_rule)  \n",
    "[Forecast Evaluation](http://www.atmos.albany.edu/daes/atmclasses/atm401/spring_2014/Scores1.pdf)  \n",
    "[Introduction to Forecast Vefrification](https://dtcenter.org/met/users/docs/presentations/WRF_Users_2012.pdf)\n",
    "[Precision Recall F1-Score Support](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support)\n",
    "[F1 SCore](https://en.wikipedia.org/wiki/F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skillScore(y_true, y_pred, skill='pss'):\n",
    "    \"\"\"\n",
    "    Compute various skill scores\n",
    "    PARAMS:\n",
    "        y_true: the true classification label\n",
    "        y_pred: the classification predicted by the model (must be binary)\n",
    "        skill: a string used to select a particular skill score to compute\n",
    "                'pss' | 'hss' | 'bss'\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    if skill == 'acc': #accuracy\n",
    "        return float(tp+tn) / (tp+fn+tn+fp)\n",
    "    if skill == 'pss':\n",
    "        tpr = float(tp) / (tp + fn)\n",
    "        fpr = float(fp) / (fp + tn)\n",
    "        pss = tpr - fpr\n",
    "        return  [pss, tpr, fpr] \n",
    "    if skill == 'hss': #Heidke\n",
    "        return 2.0 * (tp*tn - fp*fn) / ((tp+fn)*(fn+tn) + (tp+fp)*(fp+tn))\n",
    "    if skill == 'bss': #Brier Skill Score\n",
    "        return np.mean(y_true - y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTTING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, \n",
    "                          title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    PARAMS:\n",
    "        cm: the confusion matrix\n",
    "        classes: list of unique class labels\n",
    "    \"\"\"\n",
    "    # View percentages\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_mtx', bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "def plotTreeFeatImp(tree, X, feature_names):\n",
    "    \"\"\"\n",
    "    Plots bar charts of the feature importance\n",
    "    The importance of a feature is computed as the (normalized) total reduction of the \n",
    "    criterion brought by that feature. It is also known as the Gini importance\n",
    "    \"\"\"\n",
    "    n_feature = X.shape[1]\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.barh(range(0, n_feature*8, 8), tree.feature_importances_, 5, align='center')\n",
    "    plt.yticks(np.arange(0, n_feature*8, 8), feature_names)\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ROC Curve and generate the KS plot\n",
    "def ks_roc_plot(targets, predictions, FIGWIDTH=10, FIGHEIGHT=5, FONTSIZE=16):\n",
    "    ''' \n",
    "    Generate a figure that plots the ROC Curve and the distributions of the \n",
    "    TPR and FPR over a set of thresholds\n",
    "    PARAMS:\n",
    "        targets: list of true target labels\n",
    "        predictions: list of predicted labels\n",
    "    RETURNS:\n",
    "        fpr: false positive rate\n",
    "        tpr: true positive rate\n",
    "        thresholds: thresholds used for the ROC curve\n",
    "        auc: Area under the ROC Curve\n",
    "        fig, axs: corresponding handles for the figure and axis\n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(targets, predictions)\n",
    "    auc_res = auc(fpr, tpr)\n",
    "\n",
    "    # Generate KS plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(FIGWIDTH,FIGHEIGHT))\n",
    "    axs = ax.ravel()\n",
    "    ax[0].plot(thresholds, tpr, color='b')\n",
    "    ax[0].plot(thresholds, fpr, color='r')\n",
    "    ax[0].plot(thresholds, tpr - fpr, color='g')\n",
    "    ax[0].invert_xaxis()\n",
    "    ax[0].set_xlabel('threshold', fontsize=FONTSIZE)\n",
    "    ax[0].set_ylabel('fraction', fontsize=FONTSIZE)\n",
    "    ax[0].legend(['TPR', 'FPR', 'K-S Distance'], fontsize=FONTSIZE)\n",
    "    \n",
    "    # Generate ROC Curve plot\n",
    "    ax[1].plot(fpr, tpr, color='b')\n",
    "    ax[1].plot([0,1], [0,1], 'r--')\n",
    "    ax[1].set_xlabel('FPR', fontsize=FONTSIZE)\n",
    "    ax[1].set_ylabel('TPR', fontsize=FONTSIZE)\n",
    "    ax[1].set_aspect('equal', 'box')\n",
    "    auc_text = ax[1].text(.05, .95, \"AUC = %.4f\" % auc_res, \n",
    "                          color=\"k\", fontsize=FONTSIZE)\n",
    "    #print(\"AUC:\", auc_res)\n",
    "\n",
    "    return fpr, tpr, thresholds, auc_res, fig, axs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS FOR COMPUTING AVERAGE ACCURACY FOR VARIOUS SUBSETS OF THE TRAINING DATA\n",
    "##### PART (b) continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSizes(estimator, X, y, sizes):\n",
    "    \"\"\"\n",
    "    PARAMS:\n",
    "        estimator: model to learn\n",
    "        X: all feature data\n",
    "        y: all labels\n",
    "        sizes: list of test sizes, as percentage of whole data\n",
    "    \"\"\"\n",
    "    train_test_acc = np.empty((0,2), float)\n",
    "\n",
    "    for tsize in sizes:\n",
    "        ''' \n",
    "        Note: the term estimater and model are used interchangebly\n",
    "        Note: the underscores acts as blanks for you to replace with your code. Make sure to remove the \n",
    "              underscores once you've completed the TODO\n",
    "        '''\n",
    "        # 4.1. TODO : partition the data (X and y) into training and testing sets\n",
    "        #        see documentation for train_tes_split() at\n",
    "        #        https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, \n",
    "                                                            test_size=tsize)\n",
    "\n",
    "        # 4.2. TODO: Train model, use estimator.fit(X_train, y_train)\n",
    "        estimator.fit(X_train, y_train)\n",
    "\n",
    "        # 4.3. TODO: Compute the predictive score for the training set\n",
    "        trng_acc = estimator.score(X_train, y_train)\n",
    "        # 4.4. TODO: Compute the predictive score for the testing set\n",
    "        test_acc = estimator.score(X_test, y_test)\n",
    "\n",
    "        train_test_acc = np.append(train_test_acc, np.array([[trng_acc, \n",
    "                                                              test_acc]]), axis=0)\n",
    "\n",
    "    return train_test_acc\n",
    "\n",
    "\n",
    "def avgTrainSizes(estimator, X, y, sizes, nsamples=25):\n",
    "    \"\"\"\n",
    "    Average and plot the accuracy for each size in sizes\n",
    "    PARAMS:\n",
    "        estimator: model to learn\n",
    "        X: all feature data\n",
    "        y: all labels\n",
    "        sizes: list of test sizes, as percentage of whole data\n",
    "        nsamples: number of times to compute the accuracy for each size\n",
    "    \"\"\"\n",
    "    train_test_acc = np.zeros((len(sizes),2), float)\n",
    "\n",
    "    for samp in range(nsamples):\n",
    "        ttacc = trainSizes(estimator, X, y, sizes)\n",
    "        train_test_acc += ttacc\n",
    "\n",
    "    train_test_acc = train_test_acc / nsamples\n",
    "\n",
    "    plt.plot(sizes, train_test_acc[:,0], 'o-', color=\"b\", label=\"train\")\n",
    "    plt.plot(sizes, train_test_acc[:,1], 'o--', color=\"r\", label=\"test\")\n",
    "    plt.title(\"Average Accuracy\")\n",
    "    plt.xlabel(\"Test Size (%)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    return train_test_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRID SEARCH FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pss(true, preds):\n",
    "    pss, tpr, fpr = skillScore(true, preds, skill='pss')\n",
    "    return pss\n",
    "\n",
    "\n",
    "def performGridSearchCV(estimator, param_grid, X, y, scoring={'PSS':make_scorer(pss)}, \n",
    "                        refit='PSS', cv=10, test_size=0.25, cvalfunc=None):\n",
    "    \"\"\"\n",
    "    PARAMS:\n",
    "        estimator: the model to build\n",
    "        param_grid: space of parameters to search\n",
    "        X: all feature data \n",
    "        y: all target data \n",
    "        refit: the name of the function to use for scoring. these functions must return \n",
    "               only 1 value (default PSS)\n",
    "        cv: number of cross validation folds\n",
    "        test_size: proportion of the data to hold out for testing\n",
    "        scoring: See https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "                 sets the scoring function to pss to compare models \n",
    "    \"\"\"\n",
    "    clf = GridSearchCV(estimator, param_grid, cv=cv, scoring=scoring, refit=refit, iid=True)\n",
    "\n",
    "    # Hold out a subset of the data for testing, before training, usimg cross validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=test_size)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found:\")\n",
    "    print(clf.best_params_)\n",
    "\n",
    "    print(\"\\nGrid scores:\")\n",
    "    means = clf.cv_results_['mean_test_%s' % refit]\n",
    "    stds = clf.cv_results_['std_test_%s' % refit]\n",
    "    dets = [] # used for tracking the order of the models by mean score\n",
    "    for i, (mean, std, params) in enumerate(zip(means, stds, clf.cv_results_['params'])):\n",
    "        print(\"%03d: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n",
    "        dets.append((mean, i))\n",
    "    dets.sort()\n",
    "    means.sort()\n",
    "    \n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(\"The model is trained on the full set using the best parameters.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    # Plotting\n",
    "    cmtx = confusion_matrix(y_true, y_pred)\n",
    "    plot_confusion_matrix(cmtx, target_names)\n",
    "    \n",
    "    if not (cvalfunc is None):\n",
    "        scores = cross_val_predict(clf.best_estimator_, X_test, y_test, \n",
    "                                   cv=cv, method=cvalfunc) #decision_function\n",
    "        fpr, tpr, thresholds, auc_res, fig, axs = ks_roc_plot(y_test, scores) ###\n",
    "        print(\"AUC:\", auc_res)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(means)\n",
    "    plt.ylabel('Mean %s'% refit)\n",
    "    plt.xlabel('Parameter Set #')\n",
    "    plt.title('')\n",
    "    plt.savefig('mean_%s_plot' % refit, bbox_inches=\"tight\")\n",
    "    \n",
    "    return dets, clf\n",
    "\n",
    "\n",
    "def performGridRegressionSearchCV(estimator, paramsdict, thres, X, y, nsplits=3, test_size=0.25):\n",
    "    \"\"\"\n",
    "    Grid search for regression. \n",
    "    Optimizes over the parameter space and a set of decision thresholds for classification\n",
    "    PARAMS:\n",
    "        estimator: type of model \n",
    "        paramsdict: space of parameters to search\n",
    "        thres: set of thresholds to use\n",
    "        X: all feature data \n",
    "        y: all target data \n",
    "        nsplits: number of cross validation folds\n",
    "        test_size: proportion of the data to hold out for testing\n",
    "    \"\"\"\n",
    "    paramnames = paramsdict.keys()\n",
    "    paramvals = paramsdict.values()\n",
    "    \n",
    "    # Determine all the permutations of parameters\n",
    "    allparamsets = list(product(*paramvals))\n",
    "    nprmsets = len(allparamsets)\n",
    "    #thres = np.arange(-.1,.6,.04)\n",
    "\n",
    "    models_tpr = np.zeros((nprmsets,nsplits,len(thres)), float)\n",
    "    models_fpr = np.zeros((nprmsets,nsplits,len(thres)), float)\n",
    "    models_auc = np.zeros((nprmsets,nsplits), float)\n",
    "\n",
    "    # Perform stratified k fold cross validation\n",
    "    skf = StratifiedKFold(n_splits=nsplits)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=test_size)\n",
    "    for s, (train, val) in enumerate(skf.split(X_train, y_train)):\n",
    "        for p, paramset in enumerate(allparamsets):\n",
    "            currparams = dict( zip(paramnames, paramset) )\n",
    "\n",
    "            # Train\n",
    "            estimator.set_params(**currparams)\n",
    "            estimator.fit(X_train[train,:], y_train[train])\n",
    "\n",
    "            # Validate the Model\n",
    "            preds = estimator.predict(X_train[val,:])\n",
    "            models_auc[p,s] = roc_auc_score(y_train[val], preds)\n",
    "\n",
    "            # Determine the labels for different thresholds\n",
    "            inds = [np.where(preds > t)[0] for t in thres]\n",
    "            # Compute PSS for different thresholds\n",
    "            for t, i in enumerate(inds):\n",
    "                prd = np.zeros(preds.shape, float)\n",
    "                prd[i] = 1 \n",
    "                pss, tpr, fpr = skillScore(y_train[val], prd, skill='pss')\n",
    "                models_tpr[p,s,t] = tpr\n",
    "                models_fpr[p,s,t] = fpr\n",
    "\n",
    "    # Compute average PSS across the folds for each threshold for all models\n",
    "    psses = models_tpr - models_fpr\n",
    "    avg_pss = np.mean(psses, axis=1)\n",
    "    avg_auc = np.mean(models_auc, axis=1)\n",
    "    avg_tpr = np.mean(models_tpr, axis=1)\n",
    "    avg_fpr = np.mean(models_fpr, axis=1)\n",
    "\n",
    "    mIdx = np.argmax(avg_auc) # get the index of the best parameter set\n",
    "    best_params = dict( zip(paramnames, allparamsets[mIdx])) # constructs a parameter dict\n",
    "    tIdx = np.argmax(avg_pss[mIdx]) # get the decision threshold\n",
    "\n",
    "    # ROC axes\n",
    "    fpr = avg_fpr[mIdx,:]\n",
    "    tpr = avg_tpr[mIdx,:]\n",
    "\n",
    "    #plt.figure()\n",
    "    #plt.plot(fpr, tpr)\n",
    "    #plt.title('ROC Curve')\n",
    "    #plt.ylabel('Average TPR')\n",
    "    #plt.xlabel('Average FPR')\n",
    "    #plt.savefig('bestparams_roc', bbox_inches=\"tight\")\n",
    "    #plt text of AUC on plot\n",
    "\n",
    "    # Construct model from all the training data, using the best parameters\n",
    "    estimator.set_params(**best_params)\n",
    "    estimator.fit(X_train, y_train)\n",
    "    \n",
    "    # Test the model and make the predictions binary\n",
    "    preds = estimator.predict(X_test)\n",
    "    preds[preds > thres[tIdx]] = 1\n",
    "    preds[preds <= thres[tIdx]] = 0\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cmtx = confusion_matrix(y_test, preds)\n",
    "    plot_confusion_matrix(cmtx, target_names)\n",
    "    \n",
    "    # Training statistics for the best threshold and parameter set\n",
    "    b_avg_pss = np.mean(models_tpr[mIdx,:,tIdx] - models_fpr[mIdx,:,tIdx])\n",
    "    b_std_pss = np.std(models_tpr[mIdx,:,tIdx] - models_fpr[mIdx,:,tIdx])\n",
    "    \n",
    "    print (\"Best Parameters\")\n",
    "    print (best_params)\n",
    "    print (\"Average Training PSS %.04f\" % b_avg_pss)\n",
    "    print (\"STD Training PSS %.04f\" % b_std_pss)\n",
    "    print (\"Best Model Test PSS %.04f\" % skillScore(y_test, preds, 'pss')[0])\n",
    "    print (\"Best Model Test Accuracy %.04f\" % skillScore(y_test, preds, 'acc'))\n",
    "    \n",
    "    return estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1 (a)\n",
    "Simple training and testing of a standard model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PART 1 (a)\n",
    "# 1. TODO: create a SVC model that uses a linear kernel and gamma set to 'auto'\n",
    "t0 = time.time()\n",
    "svc_a = # TODO\n",
    "\n",
    "# 2. TODO: Split the data into training and testing sets.  ****\n",
    "#    Use train_test_split\n",
    "#    Select a portion to hold out for testing\n",
    "#    Additionally, stratify the splits\n",
    "#    see the documentation for more details at\n",
    "#    https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split\n",
    "X_train, X_test, y_train, y_test = # TODO\n",
    "\n",
    "# 3. TODO: Train the model using the training data and fit\n",
    "\n",
    "\n",
    "# 4. TODO: Computing the predictive accuracy of the train set\n",
    "trng_acc = # TODO\n",
    "# 5. TODO: Computing the predictive accuracy of the test set\n",
    "test_acc = # TODO\n",
    "\n",
    "t1 = time.time()\n",
    "et = t1 - t0\n",
    "# Print out the training accuracy, testing accuracy, and the portion of the data used for testing\n",
    "print (\"Train Accuracy %.04f\" % trng_acc)\n",
    "print (\"Test Accuracy %.04f\" % test_acc)\n",
    "print (\"Test portion .25\")\n",
    "print (\"Elapsed time: %.03f min\" % (et / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1 (b)\n",
    "Explore the impact of using different portions of your data set for training and testing.\n",
    "Instead of trying various values for test_size manually, we will use a function to try a range of values.\n",
    "Because there can be variation in the accuracy from using different subsets of the data for training, we will compute the average accuracy for each portion as well.\n",
    "\n",
    "You will use the *avgTrainSize* function for this purpose (also see *trainSizes* function). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PART 1 (b)\n",
    "# 1. TODO: create the SVC model with a linear kernel and gamma set tot 'auto'\n",
    "t0 = time.time() # get the start time this cell is run\n",
    "svc_b = # TODO\n",
    "\n",
    "# 2. TODO: adjust the test size range as needed\n",
    "tsizes = # TODO\n",
    "\n",
    "# 3. TODO: Pass the model into the function. \n",
    "#    Feel free to adjust the nsamples as you see fit. This is the number of times the accuracy\n",
    "#    is computed for the same test set portion size, but instead using a different training set\n",
    "#    to learn the model\n",
    "avgTrainSizes( # TODO ) \n",
    "\n",
    "# 4. Go to the function trainSizes, defined above and read it\n",
    "\n",
    "et = time.time() - t0 # compute the end time\n",
    "print (\"Elapsed time: %.03f min\" % (et / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decision Trees\n",
    "# 1. TODO: create DecisionTreeClassifier\n",
    "t0 = time.time()\n",
    "dtc_b = DecisionTreeClassifier(max_depth=6, min_samples_split=2)\n",
    "\n",
    "# 2. TODO: adjust the test size range as needed\n",
    "tsizes = np.arange(.05, 1, .05) # nsamples=30  0.038 min\n",
    "\n",
    "# 3. TODO: Pass the model into the function. \n",
    "#    Feel free to adjust the nsamples as you see fit. This is the number of times the accuracy\n",
    "#    is computed for the same test set portion size, but instead using a different training set\n",
    "#    to learn the model\n",
    "avgTrainSizes(dtc_b, X, y, tsizes, nsamples=30)\n",
    "\n",
    "# 4. Compare the plots for SVCs and DecisionTrees\n",
    "\n",
    "et = time.time() - t0 # compute the end time\n",
    "print (\"Elapsed time: %.03f min\" % (et / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION - GRID SEARCH AND CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2 (a)\n",
    "Try a few (about 3) combinations of parameters for the SVC, DecisionTree, or RandomForest models.\n",
    "For the SVR or SGDRegressor, since these are regressive technique, a continous value is returned from prediction instead of a class label. You will need to select a threshold, such that any predictions exceeding the threshold hold will be given label 1, otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PART 2 (a) CLASSIFICATION\n",
    "# 1. Create a SVC model with some parameter configuration\n",
    "#    See the parameters list https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "t0 = time.time() \n",
    "svc_p2 = SVC(kernel='linear', gamma='auto', C=10)\n",
    "\n",
    "# 2. Use the plot from PART 1 to determine a reasonable portion of the data to hold out for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "\n",
    "# 3. Train the model on the train set\n",
    "svc_p2.fit(X_train, y_train)\n",
    "\n",
    "# 4. TODO: Test the model's predictive ability with the test set\n",
    "preds = # TODO\n",
    "\n",
    "# 5. TODO: Evalutate the model using the Pierce Skill Score (PSS) \n",
    "#    Pass to the function the true and the predicted labels\n",
    "pss_test, _, _ = skillScore( # TODO)\n",
    "\n",
    "# Compute predictive abilities on the training set. Not a good metric as the model is overfit\n",
    "preds_train = svc_p2.predict(X_train)\n",
    "pss_train, _, _ = skillScore(y_train, preds_train, skill='pss')\n",
    "\n",
    "# 6. TODO: Construct and draw a Confusion Matrix\n",
    "#    This is used to observe the relationship between correct and incorrect predictions relative to class\n",
    "#    Pass to the function the true and the predicted labels\n",
    "cmtx = confusion_matrix( # TODO )\n",
    "plot_confusion_matrix(cmtx, target_names)\n",
    "et = time.time() - t0 # compute the end time\n",
    "\n",
    "# Print out the pss result\n",
    "print (\"Train PSS %.04f\" % pss_train)\n",
    "print (\"TEst PSS %.04f\" % pss_test)\n",
    "print (\"Elapsed time: %.03f min\" % (et / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2 (b)\n",
    "Repeat (a) for regression models such as the SVR.\n",
    "For prediction regression models predict a continous value instead of a class label.\n",
    "You will need to select a threshold, such that any predictions exceeding the threshold hold wiill be given label 1, otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PART 2 (b) CLASSIFICATION w REGRESSION MODEL\n",
    "# 1. TODO: Repeat the TODO steps 1 to 3 from (a), the SVR model with linear kernel and C=.8\n",
    "# Parameters list, https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR\n",
    "# 1.1. TODO: create a SVR model with some parameter configuration\n",
    "#    See the parameters list https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "t0 = time.time()\n",
    "svr_2b = # TODO\n",
    "\n",
    "# 1.2. Use the plot from PART 1 to determine a reasonable portion of the data to hold out for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y)\n",
    "\n",
    "# 1.3. Train the model on the train set\n",
    "svr_2b.fit(X_train, y_train)\n",
    "\n",
    "# 1.4. Test the model with the test set\n",
    "preds = svr_2b.predict(X_test)\n",
    "\n",
    "# Observe the range of predicted values\n",
    "print (\"%.04f to %.04f\" % (np.min(preds), np.max(preds)))\n",
    "\n",
    "# 2. From the range printed above, pick a value in between to act as a deciding threshold.\n",
    "thres = .9 # not optimal choice\n",
    "\n",
    "# 3. TODO: Determine the label. If the vale is > thres, set the label to 1, otherwise it's 0.\n",
    "#    [int(p > thres) for p in preds]\n",
    "preds_labels = # TODO\n",
    "\n",
    "# 5. TODO: Compute the pss\n",
    "#    Pass to the function the true and the predicted labels\n",
    "pss, _, _ = skillScore( # TODO)\n",
    "\n",
    "# Confusion Matrix\n",
    "cmtx = confusion_matrix(# TODO) # NOTE: must use the binary labels\n",
    "plot_confusion_matrix(cmtx, target_names)\n",
    "et = time.time() - t0\n",
    "    \n",
    "# Print the pss, and threshold value\n",
    "print (\"PSS %.04f using %.02f\" % (pss, thres))\n",
    "print (\"Elapsed Time: %.03f min\" % (et / 60)) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2 (c)\n",
    "Perform an exhaustive search of all permutations of a set of parameters and thresholds using the functions *performGridSearchCV*, for classification models and *performGridRegressionSearchCV* for regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PART 2 (c) CLASSIFICATION\n",
    "# construct a model with no parameters\n",
    "t0 = time.time()\n",
    "sgd_model = SGDClassifier()\n",
    "\n",
    "# 1. TODO: Select parameters and values to explore for the model. Make adjustments as you see fit\n",
    "param_grid = {'loss': ['log', 'perceptron'], 'penalty':['l2', 'l1', 'elasticnet'], \n",
    "              'max_iter':[1e4], 'tol':[1e-3]} # See documentation\n",
    "\n",
    "# 2. TODO: Pass the model and then the parameter options into performGridSearchCV \n",
    "#    Adjust cv as you see fit (note: cv should by > 2)\n",
    "#    Adjust test_size as you see fit\n",
    "X_train, X_test, y_train, y_test = train_test_split(# TODO)\n",
    "dets, clf = performGridSearchCV(sgd_model, param_grid, # TODO, # TODO, \n",
    "                                cv=20, test_size=.2)\n",
    "\n",
    "# 3. TODO: Cross validation on test performance of the best estimator\n",
    "preds = cross_val_predict( #TODO, #TODO, #TODO, cv=20, method='decision_function')\n",
    "fpr, tpr, thresholds, auc_res, fig, axs = ks_roc_plot(y_test, preds) \n",
    "\n",
    "et = time.time() - t0\n",
    "print (\"Elapsed time: %.03f min\" % (et / 60))\n",
    "\n",
    "# The list of parameters can be obtained with \n",
    "# >> for i, params in enumerate(clf.cv_results_['params']):\n",
    "# >>     print(\"%03d: %r\" % (i, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PART 2 (c)\n",
    "# Construct a model with no parameters\n",
    "t0 = time.time()\n",
    "svc_model = SVC()\n",
    "\n",
    "# 1. TODO: Select parameters and values to explore for the model. Make adjustments as you see fit\n",
    "param_grid = {'C': [.1, 10], 'gamma': np.logspace(-2, 2, 2), 'kernel':['linear','rbf']} # ~1 min\n",
    "\n",
    "\n",
    "# 2. TODO: Pass the model and then the parameter options into performGridSearchCV \n",
    "#    Adjust cv as you see fit (note: cv should by > 2)\n",
    "#    Adjust test_size as you see fit\n",
    "X_train, X_test, y_train, y_test = train_test_split(# TODO)\n",
    "dets, clf = performGridSearchCV(# TODO)\n",
    "\n",
    "# Cross validation on test performance of the best estimator\n",
    "preds = cross_val_predict(# TODO, cv=20, method='decision_function')\n",
    "fpr, tpr, thresholds, auc_res, fig, axs = ks_roc_plot(y_test, preds) ###\n",
    "\n",
    "\n",
    "et = time.time() - t0\n",
    "print (\"Elapsed time: %.03f min\" % (et / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECISION TREE CLASSIFIER - GRID SEARCH, CROSS VALIDATION AND FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TODO: Find an optimal Decision Tree\n",
    "# construct a model with no parameters\n",
    "t0 = time.time()\n",
    "tree_model = DecisionTreeClassifier()\n",
    "\n",
    "# 1. TODO: Select parameters and values to explore for the model. Make adjustments as you see fit\n",
    "# grid search for Trees is fast, they can choose a lot of parameters here\n",
    "param_grid = {'max_depth':[5, 8, 12, 25],'min_samples_split':[3, 5, 7, 10, 20]} \n",
    "\n",
    "\n",
    "# 2. TODO: Pass the model and then the parameter options into performGridSearchCV \n",
    "#    Adjust cv as you see fit (note: cv should by > 2)\n",
    "#    Adjust test_size as you see fit\n",
    "X_train, X_test, y_train, y_test = train_test_split(# TODO)\n",
    "dets, clf = performGridSearchCV(# TODO, cv=20, test_size=.2)\n",
    "\n",
    "# Evaluate Generalized performance\n",
    "preds = cross_val_predict(# TODO, cv=20) \n",
    "fpr, tpr, thresholds, auc_res, fig, axs = ks_roc_plot(y_test, preds) \n",
    "\n",
    "# Feature Importance\n",
    "plotTreeFeatImp(clf.best_estimator_, X, feature_names) \n",
    "et = time.time() - t0\n",
    "print (\"Elapsed time: %.03f min\" % (et / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOM FOREST -  GRID SEARCH, CROSS VALIDATION AND FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Find an optimal Random Forest\n",
    "t0 = time.time()\n",
    "forest_model = RandomForestClassifier()\n",
    "\n",
    "# 1. Select parameters and values to explore for the model. Make adjustments as you see fit\n",
    "# grid search for Trees is fast, they can choose a lot of parameters here\n",
    "# ~1 min with cv=3; ~4 min with cv=10\n",
    "param_grid = {'n_estimators':[60,100,500],'max_depth':[5, 8, 12, 25],'min_samples_split':[3, 5, 7, 10, 20]} \n",
    "\n",
    "\n",
    "# 2. Pass the model and then the parameter options into performGridSearchCV \n",
    "#    Adjust cv as you see fit (note: cv should by > 2)\n",
    "#    Adjust test_size as you see fit\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "dets, clf = performGridSearchCV(forest_model, param_grid, X_train, y_train, cv=4, test_size=.2)\n",
    "#clf.best_params_\n",
    "\n",
    "# Evaluate Generalized performance\n",
    "preds = cross_val_predict(clf.best_estimator_, X_test, y_test, cv=4) \n",
    "fpr, tpr, thresholds, auc_res, fig, axs = ks_roc_plot(y_test, preds) \n",
    "\n",
    "# Feature Importance\n",
    "plotTreeFeatImp(clf.best_estimator_, X, feature_names) \n",
    "\n",
    "et = time.time() - t0\n",
    "print (\"Elapsed time: %.03f min\" % (et / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGRESSION  - GRID SEARCH AND CROSS VALIDATION\n",
    "Repeat (c) for regression models using *performGridRegressionSearchCV*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### REGRESSION\n",
    "# 1. Construct the  a default model with no parameters\n",
    "t0 = time.time()\n",
    "svr = SGDRegressor() #SVR\n",
    "\n",
    "# 2. TODO: Select parameters and values to explore for the model. Make adjustments as you see fit\n",
    "# select small param space such that this is faster. Small values for C tend to run faster\n",
    "#paramsdict = {'C':[.8], 'gamma':np.logspace(-2, 2, 2), 'kernel':['linear']}#,'rbf']} # for SVR()\n",
    "paramsdict = {'loss':['squared_loss', 'huber'], 'penalty':['l1','l2','elasticnet'], \n",
    "              'max_iter':[1e4], 'tol':[1e-3], 'early_stopping':[True]}\n",
    "# 3. TODO: Adjust the range of thresholds to try as you see fit\n",
    "thres = np.arange(-.1,1,.3)\n",
    "\n",
    "# 4. TODO: Pass in the model, then the parameters, then the thresholds\n",
    "#    Adjust nsplits as you see fit (note: nsplits should by > 2)\n",
    "#    Adjust test_size as you see fit\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "estimator = performGridRegressionSearchCV(#TODO ...,\n",
    "                                          nsplits=4, test_size=.2)\n",
    "\n",
    "# Evaluate Generalized performance\n",
    "preds = cross_val_predict(# TODO, cv=20)\n",
    "fpr, tpr, thresholds, auc_res, fig, axs = ks_roc_plot(y_test, preds)\n",
    "\n",
    "et = time.time() - t0\n",
    "print (\"Elapsed Time: %.03f min\" % (et / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalET = time.time() - globalStart\n",
    "print (\"Elapsed Time of Notebook: %.03f min\" % (globalET / 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
